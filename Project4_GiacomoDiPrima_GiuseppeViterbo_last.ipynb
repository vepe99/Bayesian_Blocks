{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1112bad-27f6-47e4-92bf-64a75090039f",
   "metadata": {},
   "source": [
    "<div style='background-color:#f7f7f7; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px'>\n",
    "    <center>\n",
    "        <div style='  display: block;\n",
    "  font-size: 2em;\n",
    "  font-weight: bold;  display: block;\n",
    "  font-size: 2em;\n",
    "  font-weight: bold;'>ASPA - Bayesian Blocks Algorithm and Its Applications\n",
    "        </div>\n",
    "    <center>\n",
    "        <br>\n",
    "    <i>Giacomo Di Prima, Giuseppe Viterbo</i></center>\n",
    "    <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e655e0-91ae-4b8a-a6f5-c0244a87b795",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec08d81-803b-46b6-b154-d5179c4fccac",
   "metadata": {},
   "source": [
    "The histogram representation is a powerful tool that is widely spread across many different fields, yet it is not an objective method since it heavily depends on the choice made for binning the data.\n",
    "Due to the different approaches one can use, the resulting distributions generated from the same underlying statistics could highlight different structures modifying the interpretation of the data.\n",
    "Different methods have been devised to overcome this issue:  for example, Scott’s Rule <a name=\"cite_note-1\"><sup>[1]</sup></a> and the Freedman-Diaconis Rule <a name=\"cite_note-2\"><sup>[2]</sup></a> use the number of entries and a measure of the spread of the distribution to determine the number of bins with fixed width; Knuth’s Rule <a name=\"cite_note-3\"><sup>[3]</sup></a> consider the distribution’s structure to build bins that still have the same width; the “equal population” method enforce that each bin have similar numbers of entries, resulting in varying widths, but the location of the bin edges is chosen arbitrarily.<br>\n",
    "In contrast with them the Bayesian Block (BB) algorithm <a name=\"cite_note-4\"><sup>[4]</sup></a> <a name=\"cite_note-5\"><sup>[5]</sup></a> allows the bin widths to vary and determines the bin edges based on the structure of the distribution, making it parameter free.<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_note-1) D. W. Scott, Biometrika 66, 605 (1979)<br>\n",
    "<a name=\"cite_note-2\"></a>2. [^](#cite_note-2) D. Freedman and P. Daiconis, Zeitschrift f¨ur Wahrschein-lichkeitstheorie und verwandte Gebiete 57, 453 (1981)<br>\n",
    "<a name=\"cite_note-3\"></a>3. [^](#cite_note-3) K. H. Knuth, (2006), arXiv:physics/0605197[physics.data-an]<br>\n",
    "<a name=\"cite_note-4\"></a>4. [^](#cite_note-4) J. D. Scargle, Studies in Astronomical Time Series Analysis. V. Bayesian Blocks, a New Method to Analyze Structure in Photon Counting Data, 1998, arXiv:astro-ph/9711233.<br>\n",
    "<a name=\"cite_note-5\"></a>5. [^](#cite_note-5) J. D. Scargle, J. P. Norris, B. Jackson, and J. Chiang, Astrophys. J. 764, 167 (2013), arXiv:1207.5578[astro-ph.IM]<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1966c7c5-f301-42dd-bcf6-a12e36c66ba1",
   "metadata": {},
   "source": [
    "# Bayesian Blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fd38a3-3367-42a9-a9f0-687374fdfe69",
   "metadata": {},
   "source": [
    "Each block (or bin, in the context of histograms) is consistent with a probability distribution with compact support; the entire dataset is represented by this collection of finite probability distribution functions.\n",
    "In the algorithm implementation each block is modeled with a uniform distribution which defines the so called `Piecewise Constant Model`.\n",
    "By optimizing a goodness-of-fit statistic which depends only on the input data and a regularization parameter that envelop our prior knowledge, the number of blocks and the edges of the blocks, called `change points`, can be computed. Depending on the kind of data the fitness function changes.\n",
    "\n",
    "|Data type| Description|\n",
    "|:---|:---|\n",
    "|Events (TTE)| Time at which the measurements occured|\n",
    "|Binned Events| Counts of events in time bins|\n",
    "|Point Measurements |Measurements of a quasi-continuous observable at a sequence of points in time|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad25628-e272-421f-aba3-30aba66082f5",
   "metadata": {},
   "source": [
    "The set of blocks is gapless and non-overlapping, where the first block edge is defined by the first data point, and the last block edge is defined by the last data point. A block can contain between 1 and N data points, where the sum of the contents of all the blocks must equal N . The algorithm relies on the additivity of the fitness function, and thus the fitness of a given set of blocks is equal to the sum of the fitnesses of the individual blocks. The total fitness, $F_{total}$ for a given dataset is:\n",
    "\n",
    "$$F_{total} = \\sum_{i=1}^{K} f(B_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f621a0-d71d-4056-bfc4-05b5bab6f874",
   "metadata": {},
   "source": [
    "## Fitness\n",
    "As stated before, depending of the kind of data the fitness function assumes different forms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e17e68-c179-4051-ac1a-a5c4ed12b6cc",
   "metadata": {},
   "source": [
    "### Events\n",
    "The fitness function can be obtained starting with the unbinned likelihood known as the Cash statistic <a name=\"cite_note-6\"><sup>[6]</sup></a>:\n",
    "\n",
    "$$\\large \\log L(\\theta) = \\sum_{n} \\log M (t_n, \\theta) - \\int M (t, \\theta)dt$$\n",
    "\n",
    "where M(t, θ) is a model of the time dependence of a signal.\n",
    "The integral is over the observation interval and is the expected number of events under the model.\n",
    "Since each $k$-th block is constant, and so has a single parameter $M(t, \\theta) = \\theta$, the resulting log-likelihood is:\n",
    "\n",
    "$$\\large \\log L^{(k)}(\\theta) = N^{(k)}\\log \\theta - \\theta T^{(k)}$$\n",
    "\n",
    "where $N^{(k)}$ is the number of events in the $k$-th block and $T^{(k)}$ is the length of the block.</br>\n",
    "When $\\theta =N^{(k)}/T^{(k)} $ the fitness is maximum, yielding:\n",
    "\n",
    "$$\\large \\log L^{(k)}_{\\max}(\\theta) +  N^{(k)} = N^{(k)}(\\log N^{(k)} - \\log T^{(k)})$$\n",
    " \n",
    "Since the sum of $N^{(k)}$ is constant over $k$, the term on the left hand side is irrelevant since is model indipendent. Moreover, note that this fitness function is scale invariant $(t \\to \\alpha t)$.\n",
    "\n",
    "<a name=\"cite_note-6\"></a>6. [^](#cite_note-6) W., Parameter-Estimation in Astronomy\n",
    "through Application of the Likelihood Ratio, Astrophysical\n",
    "Journal.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a455f-fa39-40d8-966f-06388af725ac",
   "metadata": {},
   "source": [
    "### Binned Event Data\n",
    "The expected count in a bin is the product $\\lambda \\cdot e \\cdot W$ of the true event rate $\\lambda$ at the detector, a dimensionless exposure factor $e$, and\n",
    "the width of the bin $W$ . Therefore the likelihood for bin n is given\n",
    "by the Poisson distribution:\n",
    "\n",
    "$$\\large L_n = \\frac{(\\lambda e_n W_n)^{N_n}e^{- \\lambda e_n W_n}}{N_n !}$$\n",
    "\n",
    "where $N_n$ is the number of events in bin $n$, $\\lambda$ is the actual event\n",
    "rate in counts per unit time, $e_n$ is the exposure averaged over the\n",
    "bin, and $W_n$ is the bin width in time units. Defining bin efficiency\n",
    "as $w_n = e_n W_n$, the likelihood for block $k$ is the product of the\n",
    "likelihoods of all its bins:\n",
    "\n",
    "\n",
    "$$\\large L^{(k)} = \\prod_{n=1}^{M^{(k)}} L_n = \\lambda^{N^{(k)}} e^{-\\lambda w^{(k)}}$$\n",
    "\n",
    "where $M^{(k)}$ is the number of bins contained in block $k$,\n",
    "\n",
    "$$\\large w^{(k)} = \\sum_{n=1}^{M^{(k)}} w_n$$\n",
    "\n",
    "is the sum of the bin efficiencies in the block, and\n",
    "\n",
    "$$\\large N^{(k)} = \\sum_{n=1}^{M^{(k)}} N_n$$\n",
    "\n",
    "is the total event count in the block.</br>\n",
    "Notice that the factor $(e_n W_n)^{N_n}/N_n !$ has been discarted since its product over all the bins in all the blocks is constant.\n",
    "In the end the equation becomes:\n",
    "\n",
    "$$\\large \\log L^{(k)}= N^{(k)}\\log \\lambda - \\lambda w^{(k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf7b781-0387-47e3-ab6b-35798a5fa900",
   "metadata": {},
   "source": [
    "### Point Measurements\n",
    "A common experimental scenario is to measure a signal $s(t)$ at a sequence of times $t_n, n = 1, 2, . . . , N$, known as sampling, in order to characterize its time dependence.\n",
    "Inevitable corruption due to observational errors is frequently countered by smoothing the data and/or fitting a model.\n",
    "As with the other data modes Bayesian Blocks is a different approach to this issue, making use of knowledge of the observational error distribution and avoiding the information loss entailed by smoothing.\n",
    "We assume tha the measurements at these times are independent of each other, which is to say the errors of observation are statistically independent. </br>\n",
    "Generally these errors are stochastic and addittive, so the measured time series can be modeled as follow:\n",
    "\n",
    "$$\\large x_n \\equiv x(t_n) = s(t_n) + z_n$$\n",
    "\n",
    "The error $z_n$ at time $t_n$ is known only by its distribution. Let us consider $z_n \\sim \\mathcal{N}(0, \\sigma_n)$. If we model the signal as a constant $s = \\lambda$, the likelihoood of the $n$-th measurement is:\n",
    "\n",
    "$$ \\large L_n = \\frac{1}{\\sigma_n \\sqrt{2 \\pi}} e^{- \\frac{1}{2} \\left( \\frac{x_n - \\lambda}{\\sigma_n}\\right)^2}$$\n",
    "\n",
    "So, since we assume the indipendence of the measurements, the block $k$ lielihood is:\n",
    "\n",
    "$$\\large L^{(k)} = \\prod_n L_n = \\frac{(2 \\pi)^{- \\frac{N_k}{2}}}{\\prod_m \\sigma_m} e^{-\\frac{1}{2} \\sum_n \\left( \\frac{x_n - \\lambda}{\\sigma_n}\\right)^2}$$\n",
    "\n",
    "Notice that the quantity multiplying the exponential in the above equation is irrelevant because it contributes an overall constant factor to the total likelihood.</br>\n",
    "Following the computation in the original paper, the maximum value for the log-likelihood is given by:\n",
    "\n",
    "$$\\large \\log L^{(k)}_{\\max} = b_{k}^{2}/4 a_k$$\n",
    "\n",
    "where $a_k$ and $b_k$ are defined as follows:\n",
    "\n",
    "$$ a_k = \\frac{1}{2} \\sum_n \\frac{1}{\\sigma^2_n}$$\n",
    "$$ b_k = \\frac{1}{2} \\sum_n \\frac{x_n}{\\sigma^2_n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7ef1ba-9196-4ef8-807a-8874f888ac13",
   "metadata": {},
   "source": [
    "## Prior Distribution of the Number of Blocks\n",
    "\n",
    "Following the Bayes rule, we need to implement a prior probability for the number of blocks that are needed. In particular, in most settings it is much more likely a prior for which $N_{blocks} << N$, than $N_{blocks} \\approx N$, with $N$ beeing the number of data points. With this in mind, the prior that was adopted is a `geometric prior` with parameter $\\gamma$:\n",
    "\n",
    "$$P(N_{blocks}) = P_0 \\gamma^{N_{blocks}} $$\n",
    "\n",
    "where $P_0$ represent the normalization constant, and it is easily obtained as follows:\n",
    "\n",
    "$$P(N_{blocks}) = \\frac{1-\\gamma}{1-\\gamma^{N+1}} \\gamma^{N_{blocks}}$$\n",
    "and the expeceted number of blocks is:\n",
    "\n",
    "$$ <N_{blocks}> = P_0 \\sum_{N_{blocks}}^{N} N_{blocks} \\gamma^{N_{blocks}} = \\frac{N\\gamma^{N+1}+1}{\\gamma^{N+1}-1} + \\frac{1}{1-\\gamma}  $$\n",
    " \n",
    "It is possible to note that the estimated number of blocks is a discontiunuos monotonic function of $\\gamma$, and for values greater than 1 it leads almost certanily to assigning each datum to a separate block. \n",
    "\n",
    "The contribution of the prior to the fitness of each block can be implemented simply by adding the constant $log(\\gamma)$ (refered to as `ncp_prior`) to the fitness of each block as follow:\n",
    "\n",
    "$$F_{total} = \\sum_{i=1}^{K} f(B_i) = \\sum_{i=1}^{K} log L^{i} + log(\\gamma) $$\n",
    "\n",
    "The choice of $\\gamma$ influences the visual representation, adjusting the amount of structure in the blocks, so it is worth to brifly discuss its effects and how to properly choose it. In many application the results are rather insensitive to the value of $\\gamma$, as long as the signal to noise ratio is moderatly large, but for extreme value, i.e. ($\\gamma \\geq$ 1), it can lead to bad results, like too few or too many blocks. The tread off that one must consider when choising the prior is between a conservative choice for the number of blocks, resilient to flactuation but possibly missing real changes, and a liberal choice, which is able to capture more changes and also is more prone to false detections. As suggested by Scargle et al.<a name=\"cite_note-5\"><sup>[5]</sup></a>, an objective method can be implemented as a function of the number of data points $N$, based on the fact that the `ncp_prior` can be seen as a proxy for the false alarm rate $p_0$, which is defined as the frequency with which the algorithm correctlty rejects the presence of a change point. The results of a high number of simulation performed by Scargle et al<a name=\"cite_note-5\"><sup>[5]</sup></a>. for a wide range of $N$ and $p_0$, is that their relation with the `ncp_prior` is well fitted by the following formula:\n",
    "\n",
    "$$ \\text{ncp_prior}  = 4 - log(75.53 p_0 N^{-0.478})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940c95e-6ebe-4574-ac32-605f47e2b5ca",
   "metadata": {},
   "source": [
    "## The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baded1cc-6d95-41a8-85be-2033e6160ea6",
   "metadata": {},
   "source": [
    "### Description\n",
    "Given an ordered set of $N$ data points, the algorithm determines the optimal set of $K + 1$ change-points (and therefore $K$ blocks) which partition the observation intervall,  by iterating through the data points, and caching the current maximum fitness values and\n",
    "corresponding indices.\n",
    "At most there can be $N$ total blocks, one for each point.\n",
    "It can be proven that given $N$ data points the total number of possible partitions of the observation interval is $2^N$. Following the orignal work, it can be shown that removing the last block of an optimal partition leaves an optimal partition. This allows to iteratively consider an increasing number of data points and at each step compute the optimum partition for them, that will be used for the next iteration, thus reducing the space of the possible partition to check drammatically. The computational time of the algorithm is $O(N^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a2051-89b7-40d0-8889-7f959f99c096",
   "metadata": {},
   "source": [
    "<center>\n",
    "<div style='background-color:#f7f7f7; text-align: center; max-width: 1000px; max-height: 1200px; padding-top:30px; padding-left:20px; padding-right:20px; padding-bottom:30px; '>\n",
    "    <img src=\"algo.png\" width=\"900\" />\n",
    "</div>\n",
    "<em><strong>Figure 1</strong>: Representation of the iteration over the possible optimal partition of the observation intervall. The red dots represents the events, while the black ticks over the timeline indicate the possible change points.</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d26d0b-a43c-4885-9aa8-b5187184d38d",
   "metadata": {},
   "source": [
    "### Our Implementation\n",
    "\n",
    "For our implementation we have taken inspiration from both the MATLAB code reported in Scargle et al.<a name=\"cite_note-5\"><sup>[5]</sup></a> and from the Astropy source code for the Bayesian Blocks algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8301228f-d90d-4bcc-98bc-715671195391",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Validate Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e15208-f7d0-4525-9d1a-93ac4e4f1920",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "validate.input = function(data, data_type='Events'){\n",
    "    \n",
    "    cells = data\n",
    "    \n",
    "    #using the data.frame structure\n",
    "    if (!is.data.frame(cells)){\n",
    "        \n",
    "        cells = as.data.frame(cells)\n",
    "    }\n",
    "    \n",
    "   \n",
    "    if (data_type=='Events'){\n",
    "        #count the number of times an event is registered\n",
    "        cells = cells |> count(cells)\n",
    "        #sort along times\n",
    "        return(cells[order(cells[1]), ])\n",
    "    }\n",
    "    \n",
    "    if (data_type=='RegularEvents'){\n",
    "        # In each tick, there are either zero or one counts. \n",
    "        #sort along times\n",
    "        return(cells[order(cells[1]), ])\n",
    "    }\n",
    "    \n",
    "    if (data_type=='PointMeasures'){\n",
    "        #sort along times\n",
    "        return(cells[order(cells[1]), ])\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7884b7-822b-49e0-b2ed-0d42ad376ee3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compute Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ff725c-e2d7-46b1-8c6d-925b11b58bb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute.p0_prior = function(N, p0=0.05){\n",
    "    return(4 - log(73.53 * p0 * (N**-0.478))) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6685d9-3aec-4285-bb8a-db64926546a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute.ncp_prior = function(gamma=NULL, N){\n",
    "    if (is.null(gamma)){\n",
    "        return(compute.p0_prior(N))\n",
    "    }\n",
    "    else{\n",
    "        return(-log(gamma))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c4d61c-197b-4e1c-9810-652ecc6e9ee4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Compute Fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133f6606-4e3b-4391-a248-e224407ea924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute.fitness = function(count_vec, width, dt=NULL, a_k=NULL, b_k=NULL, data_type='Events'){\n",
    "    \n",
    "    if (data_type=='Events'){\n",
    "        return(count_vec*log(count_vec/width))\n",
    "    }\n",
    "    else if (data_type=='RegularEvents'){\n",
    "        M = width/dt\n",
    "        count_vec_over_M = count_vec/M\n",
    "        \n",
    "        esp = 1e-8\n",
    "        if (sum(count_vec_over_M > 1+esp) > 0){\n",
    "            cat(\"regular events: N/M > 1.  Is the time step correct?\")\n",
    "            break\n",
    "        }\n",
    "        \n",
    "        one_m_NM = 1 - count_vec_over_M\n",
    "        count_vec_over_M[count_vec_over_M <= 0] = 1\n",
    "        one_m_NM[one_m_NM <= 0] = 1\n",
    "        \n",
    "        return(count_vec * log(count_vec_over_M) + (M  - count_vec)*log(one_m_NM))\n",
    "    }\n",
    "    else if(data_type=='PointMeasures'){\n",
    "        return( (b_k*b_k)/(4*a_k) )\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d3dc1b-78c8-4973-b3d6-0e9a9b0b1204",
   "metadata": {},
   "source": [
    "#### Bayesian Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b92d5-46ba-4620-a604-506b759a2d4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bayesian_blocks = function(data, ncp_prior=NULL, gamma=NULL, dt=NULL, data_type='Events'){\n",
    "    \n",
    "    cells = validate.input(data, data_type)\n",
    "    \n",
    "    t = cells[[1]]\n",
    "    x = cells[[2]]\n",
    "    if (length(data)==3){sigma = cells[[3]]} #length(data) returns the number of columns of the dataframe \n",
    "    else{sigma = 1}\n",
    "    N = length(t)\n",
    "    \n",
    "\n",
    "   \n",
    "    #quantities needed for the computation of a_k, b_k\n",
    "    ak_raw = rep(x = 1, times=length(x))/sigma**2\n",
    "    bk_raw = x/sigma**2\n",
    "    ck_raw = (x*x)/sigma**2\n",
    "    \n",
    "    edges = c(t[1],  0.5 * (t[2:length(t)] + t[1:(length(t)-1)] ), t[length(t)])\n",
    "    block_length = t[length(t)] - edges\n",
    "    \n",
    "    best = rep(0, times = N)\n",
    "    last = rep(1, times = N)\n",
    "    \n",
    "    #-----------------------------------------------------------------\n",
    "    # Start with first data cell; add one cell at each iteration\n",
    "    #-----------------------------------------------------------------\n",
    "    \n",
    "    for (K in seq(1:N)){\n",
    "        # Compute the width and count of the final bin for all possible\n",
    "        # locations of the K^th changepoint\n",
    "        width = block_length[1:K] - block_length[K + 1] #T_K\n",
    "        count_vec = rev(cumsum(rev(x[1:K]))) #N_K\n",
    "        \n",
    "        a_k = NULL\n",
    "        b_k = NULL\n",
    "        \n",
    "        if (is.null(ncp_prior)){\n",
    "            ncp_prior = compute.ncp_prior(gamma, N)\n",
    "        }\n",
    "        \n",
    "        if (data_type !='Events' ){\n",
    "            a_k = 0.5*rev(cumsum(rev(ak_raw[1:K])))\n",
    "            b_k = -1 * rev(cumsum(rev(bk_raw[1:K])))\n",
    "        }\n",
    "        \n",
    "        \n",
    "        fit_vec = compute.fitness(count_vec = count_vec, width = width, a_k=a_k, b_k=b_k, dt=dt, data_type)\n",
    "        \n",
    "        post_vec = fit_vec - ncp_prior\n",
    "        \n",
    "        post_vec[2:length(post_vec)] = post_vec[2:length(post_vec)] + best[1:(K-1)]\n",
    "        \n",
    "        i_max = which.max(post_vec)\n",
    "        last[K] = i_max\n",
    "        best[K] = post_vec[i_max]\n",
    "    }\n",
    "    \n",
    "    #-----------------------------------------------------------------\n",
    "    # Recover changepoints by iteratively peeling off the last block\n",
    "    #-----------------------------------------------------------------\n",
    "\n",
    "    change_points =  rep(1, times=N)\n",
    "    i_cp = N\n",
    "    ind = N+1\n",
    "    \n",
    "    while (i_cp>1){\n",
    "        change_points[i_cp] = ind\n",
    "        if (ind == 1){ break }\n",
    "        i_cp = i_cp-1\n",
    "        ind = last[ind - 1]\n",
    "    }\n",
    "    \n",
    "    if (i_cp==1){change_points[i_cp] = 1}\n",
    "    change_points = change_points[i_cp:length(change_points)]\n",
    "\n",
    "    return(edges[change_points])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54980a7f-e987-4fe9-b9f3-103b1a7301f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "<img src=\"Plots/N2.png\" width=\"1000\" />\n",
    "</div>\n",
    "<em><strong>Figure 2</strong>: Algorithm computational time test. An increasing number of uniformly distributed events have been generated over the intervall $[0,100)$ and for each one of them have been computed the edges of the bayesian blocks and the time it took to finish the calculations. In order to build some statistics for every data size the algorithm has been run 10 times. The data produced have been fitted with a parabola with its vertex on the origin.</em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01f7b46-4557-4abe-a2ef-f149871725f5",
   "metadata": {},
   "source": [
    "# Synthetic Data\n",
    "The BB have been calculated  on `Events` type of data sampled from an uniform and a gaussian distribution, Figure 3. When considering the uniformly distributed samples only one block has been computed that encompass the entirety of the samples. The algorithm perfectly understands the absence of any meaningful structure, yielding the exact pdf that generated the data. As for the gaussian distributed data, the algorithm yield a perfectly reasonable representation of the underlying pdf. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a2efff-60b0-4971-9876-d736dbc2b9a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center>\n",
    "    <div>\n",
    "        <img src=\"Plots/toy_final.png\" width=\"1200\" />\n",
    "    </div>\n",
    "    <em><strong>Figure 3</strong>: The first row shows the pdfs that generated the data used to test the algorithm: on the left a uniform distribution over the intervall $[0,100)$ from which have been sampled 1000 points; on the right is plotted a gaussian distribution centered in $0$ and with $\\sigma = 0.5$. The second row shows for each pdf two histograms: in red the one built with $\\sqrt{N}$ number of fixed width bins, while in blue the one built using the bayesian blocks.\n",
    "    </em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae1ca5d-f559-4500-838b-348e430a7118",
   "metadata": {},
   "source": [
    "# Prior Dependency\n",
    "As previously explained, we can inject our knowledge of the data into the algorithm via three different parameter. In particular we explored how changing the value of $\\gamma$ in a range from $0$ to $1$ influence the computation of the changing points for a point measures data type. As can be seen from Figure 4, the number of blocks increases as the value  of $\\gamma$ grows. When approaching $1$ the obtained representation is heavlily influenced by the data and for $\\gamma = 1$ we get one block for each point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b30f197-e201-459c-9145-83be55599fda",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <div>\n",
    "        <img src=\"gamma_gif.gif\" width=\"1000\" />\n",
    "    </div>\n",
    "    <em><strong>Figure 4</strong>: The data have been generated by adding a uniformly generated noies, in the range of $[-0.05, 0.05)$, to a signal with a gaussian shape, centered in $3$, with a $\\sigma = 1$ and amplitude of $0.5$. The step line represents the bayesian block computed for the different values $\\gamma$.\n",
    "    </em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384e93b3-7c58-4cd0-8611-ac99b43ea3cc",
   "metadata": {},
   "source": [
    "# Bayesian Blocks Vs. KDE\n",
    "## Kernel Density Estimation\n",
    "Kernel density estimation (KDE) is a technique for estimation of probability density function that enable the user to have a different view on data that traditionally would be histogrammed.\n",
    "Unlike the histogram, the kernel technique produces smooth estimate of the pdf, uses all sample points' locations and more convincingly suggest multimodality<a name=\"cite_note-7\"><sup>[7]</sup></a>.\n",
    "\n",
    "\n",
    "<a name=\"cite_note-7\"></a>7. [^](#cite_note-7)Węglarczyk, Stanisław. \"Kernel density estimation and its application.\" ITM web of conferences. Vol. 23. EDP Sciences, 2018.<br>\n",
    "\n",
    "## Comparison\n",
    "Here we report the comparison between the visualizations of some synthetic data obtained with the Bayesian Block algorithm and the kernel density estimation (KDE) method.\n",
    "As can be seen from Figure 5, even though the KDE representation seems to yield the correct location of the distributions' peaks, it drammatically fails to represent correctly the variance of the most sharp one, moreover the right tail of the distribution falls while the true pdf presents a uniform behaviour.\n",
    "Meanwhile the bayesian blocks clearly represents the most sharp peak, both its position and variance, while indicating the presence on the wider one, despite beeing less clear with respect to the KDE visualization. The algorithm prefectly shows the uniform distributed reagions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086f842-3321-44e7-b579-65ed3cf3cb55",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center>\n",
    "<div>\n",
    "    <img src=\"Plots/kde_total.png\" width=\"800\" />\n",
    "</div>\n",
    "    <em><strong>Figure 5</strong>: On the top is plotted the pdf from which the data have been sampled.\n",
    "        The background is been sampled from a uniform distribution ranging from 0 to 100 for a total of 1000 points, while the two signals consists in 100 points each sampled from two gaussian distribution: the first with $\\mu = 10$ and $\\sigma=4$, while the second one with $\\mu = 40$ and $\\sigma=1$\n",
    "    </em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3417bee-b747-4656-aeaf-5a214ebaae8e",
   "metadata": {},
   "source": [
    "# Applications\n",
    "Due to the histograms' nature the algorithm can be applied any time this kind of representation is involved. In the following sections we present some results we found and recreate as well as a possible application in deonising some signals. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7d5195-3ec0-4e8d-ba48-d905ab9605f3",
   "metadata": {},
   "source": [
    "## BATSE TTE \n",
    "\n",
    "In order to test our implementation of the Bayesian Blocks algorithm we manage to succesfully reproduce the results obtained by Scargle et al.<a name=\"cite_note-5\"><sup>[5]</sup></a>, in section 4.1. In the following figure are reported the Trigger 551 events in the BATSE (Burst & Transient Source Experiment) catalog 4B. The data consists of gamma ray burst photon counts, splitted into energy channels. Even though the data that we use for this plots should be the same from Scargle et al.<a name=\"cite_note-5\"><sup>[5]</sup></a>, we note that the order of magnitude of the Counts is lower than the one reported in the article, but the overall trend is similar.\n",
    "\n",
    "<center>\n",
    "    <div>\n",
    "<img src=\"Plots/tte_551_plot.png\" width=\"1000\" />\n",
    "        </div>\n",
    "    <em><strong>Figure 6</strong>: BASTE TTE data for Trigger 551.\n",
    "    </em>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27ee853-7aaf-4962-b5c8-d473b1a46d5b",
   "metadata": {},
   "source": [
    "## Light Curves\n",
    "Following the steps of Meyer et al. <a name=\"cite_note-8\"><sup>[8]</sup></a>, we apply the Bayesian Blocks method to $\\gamma$-ray light curves obteined by the Fermi Large Area Telescope (LAT)<a name=\"cite_note-9\"><sup>[9]</sup></a>. This data are of the kind `Point Measurements`. The results provide an almost objective way of splitting the light curve into groups of quiescent and flarirng episodes.\n",
    "\n",
    "\n",
    "<a name=\"cite_note-8\"></a>8. [^](#cite_note-8) Meyer, M., Scargle, J. D., & Blandford, R. D. (2019). Characterizing the gamma-ray variability of the brightest flat spectrum radio quasars observed with the Fermi LAT. The Astrophysical Journal, 877(1), 39. <br>\n",
    "<a name=\"cite_note-9\"></a>9. [^](#cite_note-9) Data from: https://zenodo.org/record/2598791"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6a70ec-6b9c-4649-89e7-3661912f5a90",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <div>\n",
    "<img src=\"Plots/lc1.png\" width=\"1000\" />\n",
    "        </div>\n",
    "    <em><strong>Figure 7</strong>: Event 3C273\n",
    "    </em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f71a12-4e6a-4540-ad02-478724356917",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <div>\n",
    "<img src=\"Plots/lc2.png\" width=\"1000\" />\n",
    "        </div>\n",
    "    <em><strong>Figure 8</strong>: Event PKS1510\n",
    "    </em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc74fb56-0063-452a-a4bd-32d3206a27cf",
   "metadata": {},
   "source": [
    "## ECG\n",
    "Using the Bayesian Blocks on point measurements type of data we thought to test if was possible to denoise an ECG track without loosing any meaningful information. The data used comes from Kaggle <a name=\"cite_note-10\"><sup>[10]</sup></a>. As can be qualitatively seen by setting `ncp_prior` = 100, thus forcing the representation to use less blocks, the result representation computed by the algorithm perfetcly reproduce the shape of the signal, keeping its key features, while ignoring its small fluctuations and flattening the region with no informations.\n",
    "\n",
    "\n",
    "<a name=\"cite_note-10\"></a>10. [^](#cite_note-10) Data from: https://www.kaggle.com/datasets/stetelepta/sample-ecg-data?resource=download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ec071-f904-4ead-b90e-072f68ba9861",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <div>\n",
    "<img src=\"Plots/ecg.png\" width=\"1000\" />\n",
    "        </div>\n",
    "    <em><strong>Figure 9</strong>: On top is shown the ECG signal while on the botton is plotted the histogramm over the track.\n",
    "    </em>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6ce795-441d-4835-ae55-219cde2dea99",
   "metadata": {},
   "source": [
    "# Conclusion and future work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47964b05-ae78-4d3c-be7a-f49d7edb526a",
   "metadata": {},
   "source": [
    "We succeeded in implementing the bayesian blocks algorithm in R: we tested it on different kind of data, both synthetic and real ones as well of different kinds, and it yielded  insightful representations of them, keeping the key feature of their structures while flattening uniformly distributed regions of the observation space. The algorithm yielded better results in resolving the variance of signals affected by uniform background noise with respect to the KDE method. Its dependency from the prior knowledge, coded as a tunable parameter, results being really useful in order to simplify the data representation while still keeping track of the key element of the underlying distribution. \n",
    "Further development on the algorithm will be to implement it in such a way that will enable to study multivariate data as well further investigate for which application this method represent a better visualization with respect to the KDE and vice versa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
